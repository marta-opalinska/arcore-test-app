/*
 * Copyright 2021 Google LLC
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import android.graphics.Bitmap
import android.media.Image
import android.opengl.GLES30
import com.example.testactivity_arcoreissue.render.MainRender
import com.example.testactivity_arcoreissue.render.Mesh
import com.example.testactivity_arcoreissue.render.Shader
import com.example.testactivity_arcoreissue.render.Texture
import com.example.testactivity_arcoreissue.render.VertexBuffer
import com.example.testactivity_arcoreissue.ui.ARCircleDetectionData
import com.google.ar.core.Coordinates2d
import com.google.ar.core.Frame
import com.google.ar.core.exceptions.DeadlineExceededException
import com.google.ar.core.exceptions.NotYetAvailableException
import java.io.IOException
import java.nio.ByteBuffer
import java.nio.ByteOrder

/**
 * This class both renders the AR camera background and composes the a scene foreground. The camera
 * background can be rendered as either camera image data or camera depth data. The virtual scene
 * can be composited with or without depth occlusion.
 */
class BackgroundRenderer(render: MainRender?) {
    private val cameraTexCoords =
        ByteBuffer.allocateDirect(COORDS_BUFFER_SIZE).order(ByteOrder.nativeOrder()).asFloatBuffer()
    private val mesh: Mesh
    private val cameraTexCoordsVertexBuffer: VertexBuffer
    private var backgroundShader: Shader? = null
    private val occlusionShader: Shader? = null
    private var showDepth = false
    private lateinit var latestDepth: Image
    private lateinit var latesConfidence: Image
    private var render = render

    /** Return the camera depth texture generated by this object.  */
    val cameraDepthTexture: Texture

    /** Return the camera color texture generated by this object.  */
    val cameraColorTexture: Texture
    private var useDepthVisualization = false
    private val useOcclusion = false
    private var aspectRatio = 0f
    private var latestFrameID: Long = 0

    /**
     * Allocates and initializes OpenGL resources needed by the background renderer. Must be called
     * during a [SampleRender.Renderer] callback, typically in [ ][SampleRender.Renderer.onSurfaceCreated].
     */
    init {
        cameraColorTexture = Texture(
            render,
            Texture.Target.TEXTURE_EXTERNAL_OES,
            Texture.WrapMode.CLAMP_TO_EDGE, /*useMipmaps=*/
            false
        )
        cameraDepthTexture = Texture(
            render,
            Texture.Target.TEXTURE_2D,
            Texture.WrapMode.CLAMP_TO_EDGE, /*useMipmaps=*/
            false
        )

        // Create a Mesh with three vertex buffers: one for the screen coordinates (normalized device
        // coordinates), one for the camera texture coordinates (to be populated with proper data later
        // before drawing), and one for the virtual scene texture coordinates (unit texture quad)
        val screenCoordsVertexBuffer =
            VertexBuffer(render, /* numberOfEntriesPerVertex=*/2, NDC_QUAD_COORDS_BUFFER)
        cameraTexCoordsVertexBuffer =
            VertexBuffer(render, /*numberOfEntriesPerVertex=*/2, /*entries=*/null)
        val virtualSceneTexCoordsVertexBuffer =
            VertexBuffer(render, /* numberOfEntriesPerVertex=*/2, VIRTUAL_SCENE_TEX_COORDS_BUFFER)
        val vertexBuffers = arrayOf(
            screenCoordsVertexBuffer, cameraTexCoordsVertexBuffer, virtualSceneTexCoordsVertexBuffer
        )
        mesh = Mesh(render, Mesh.PrimitiveMode.TRIANGLE_STRIP, /*indexBuffer=*/null, vertexBuffers)
    }

    /**
     * Sets whether the background camera image should be replaced with a depth visualization instead.
     * This reloads the corresponding shader code, and must be called on the GL thread.
     */
    @Throws(IOException::class)
    fun setUseDepthVisualization(render: MainRender?, useDepthVisualization: Boolean) {
        if (backgroundShader != null) {
            if (this.useDepthVisualization == useDepthVisualization) {
                return
            }
            backgroundShader!!.close()
            backgroundShader = null
            this.useDepthVisualization = useDepthVisualization
        }
        backgroundShader = if (useDepthVisualization) {
            Shader.createFromAssets(
                render,
                "shaders/background_show_depth_color_visualization.vert",
                "shaders/background_show_depth_color_visualization.frag", /*defines=*/
                null
            )
                .setTexture("u_CameraDepthTexture", cameraDepthTexture)
                .setDepthTest(false)
                .setDepthWrite(false)
        } else {
            Shader.createFromAssets(
                render,
                "shaders/background_show_camera.vert",
                "shaders/background_show_camera.frag", /*defines=*/
                null
            )
                .setTexture("u_CameraColorTexture", cameraColorTexture)
                .setDepthTest(false)
                .setDepthWrite(false)
        }
    }

    /**
     * Updates the display geometry. This must be called every frame before calling either of
     * BackgroundRenderer's draw methods.
     *
     * @param frame The current `Frame` as returned by [Session.update].
     */
    fun updateDisplayGeometry(frame: Frame) {
        if (frame.hasDisplayGeometryChanged()) {
            // If display rotation changed (also includes view size change), we need to re-query the UV
            // coordinates for the screen rect, as they may have changed as well.
            frame.transformCoordinates2d(
                Coordinates2d.OPENGL_NORMALIZED_DEVICE_COORDINATES,
                NDC_QUAD_COORDS_BUFFER,
                Coordinates2d.TEXTURE_NORMALIZED,
                cameraTexCoords
            )
            cameraTexCoordsVertexBuffer.set(cameraTexCoords)
        }
    }

    fun saveFrame(
        frame: Frame,
        sessionSaver: SessionSaver,
        currentFrameAsBitmap: Bitmap,
        currentFullFrameAsBitmap: Bitmap?,
        arCircleDetectionData: MutableList<ARCircleDetectionData>
    ): Boolean {
        @Suppress("SwallowedException")
        try {
            if (isDepthAvailable(frame)) {
                sessionSaver.setDepthAndConfidence(
                    latestDepth,
                    latesConfidence
                )

                if (currentFullFrameAsBitmap != null) {
                    sessionSaver.setFullFrameBitmap(currentFullFrameAsBitmap)
                    sessionSaver.saveFullFrameBitmap()
                }
                sessionSaver.saveDepth()
                sessionSaver.setBitmap(currentFrameAsBitmap)
                sessionSaver.savePicture()
                sessionSaver.saveCircleIdentificationInfoJSON(arCircleDetectionData)
                return true
            }
            return false
        } catch (e: IOException) {
            return false
        }
    }

    fun update(frame: Frame) {
        this.updateDisplayGeometry(frame)
        if (this.showDepth) {
            this.updateCameraDepthTexture(frame)
        }
    }

    /** Update depth texture with Image contents.  */
    fun updateCameraDepthTexture(frame: Frame) {
        if (isDepthAvailable(frame)) {
            GLES30.glBindTexture(GLES30.GL_TEXTURE_2D, cameraDepthTexture.textureId)
            GLES30.glTexImage2D(
                GLES30.GL_TEXTURE_2D,
                0,
                GLES30.GL_RG8,
                latestDepth.width,
                latestDepth.height,
                0,
                GLES30.GL_RG,
                GLES30.GL_UNSIGNED_BYTE,
                latestDepth.planes[0].buffer
            )
            if (useOcclusion) {
                aspectRatio = latestDepth.width.toFloat() / latestDepth.height.toFloat()
                occlusionShader!!.setFloat("u_DepthAspectRatio", aspectRatio)
            }
        }
    }

    /**
     * Draws the AR background image. The image will be drawn such that virtual content rendered with
     * the matrices provided by [com.google.ar.core.Camera.getViewMatrix] and
     * [com.google.ar.core.Camera.getProjectionMatrix] will
     * accurately follow static physical objects.
     */
    fun drawBackground(render: MainRender) {
        render.draw(mesh, backgroundShader)
    }

    fun isDepthAvailable(frame: Frame): Boolean {
        @Suppress("SwallowedException")
//       Checks if the frame was saved previously
        return try {
            if (latestFrameID == frame.timestamp) {
                return true
            }
            // saving the resources
            if (this::latestDepth.isInitialized) {
                latestDepth.close()
                latesConfidence.close()
            }
            latestDepth = frame.acquireRawDepthImage16Bits()
            latesConfidence = frame.acquireRawDepthConfidenceImage()
            latestFrameID = frame.timestamp
            true
        } catch (e: NotYetAvailableException) {
            false
        } catch (e: DeadlineExceededException) {
            false
        }
    }

    fun getTexture(showDepth: Boolean): Int {
        // if the depth is shown then use depth texture
        this.showDepth = showDepth
        if (showDepth) {
            return this.cameraDepthTexture.textureId
        }
        return this.cameraColorTexture.textureId
    }

    fun getLocationOnScreen(): IntArray {
        return this.render!!.loc
    }

    fun getRawDepthAndConfidenceCoordinates(frame: Frame, x: Int, y: Int): List<Float>? {
        val results: MutableList<Float> = ArrayList()
        val cpuCoordinateX = x.toFloat()
        val cpuCoordinateY = y.toFloat()
        val cpuCoordinates = floatArrayOf(cpuCoordinateX, cpuCoordinateY)
        //        Coordinates2d.VIEW
        val textureCoordinates = FloatArray(2)
        frame.transformCoordinates2d(
            Coordinates2d.VIEW,
            cpuCoordinates,
            Coordinates2d.TEXTURE_NORMALIZED,
            textureCoordinates
        )

        if (textureCoordinates[0] < 0 || textureCoordinates[1] < 0) {
            // There are no valid depth coordinates, because the coordinates in the CPU image are in the
            // cropped area of the depth image.
            results.add(ERROR_VALUE)
            results.add(ERROR_VALUE)
            return results
        }
        val deepX: Float = textureCoordinates[0]
        val deepY: Float = textureCoordinates[1]

        return getRawDepthAndConfidence(frame, deepX, deepY)
    }

    @Suppress("SwallowedException")
    private fun getRawDepthAndConfidence(
        frame: Frame,
        textureCoordinateX: Float,
        textureCoordinateY: Float
    ): List<Float>? {
        // The depth image has a single plane, which stores depth for each
        // pixel as 16-bit unsigned integers.
        val results: MutableList<Float> = ArrayList()
        return try {
            val depthImage = frame.acquireRawDepthImage16Bits() // acquireDepthImage16Bits();
            val confidenceImage = frame.acquireRawDepthConfidenceImage()
            val deepX = (textureCoordinateX * depthImage.width).toInt()
            val deepY = (textureCoordinateY * depthImage.height).toInt()
            val depthPlane = depthImage.planes[0]
            val confidencePlane = confidenceImage.planes[0]
            val byteIndex = deepX * depthPlane.pixelStride + deepY * depthPlane.rowStride
            val bufferDepth = depthPlane.buffer.order(ByteOrder.nativeOrder())
            val confidenceBufferOriginal = confidencePlane.buffer
            val confidenceBuffer = ByteBuffer.allocate(confidenceBufferOriginal.capacity())
            confidenceBuffer.order(ByteOrder.LITTLE_ENDIAN)
            while (confidenceBufferOriginal.hasRemaining()) {
                confidenceBuffer.put(confidenceBufferOriginal.get())
            }
            confidenceBuffer.rewind()
            // Only the lowest 13 bits are used to represent depth in millimeters.
            val depthSample = bufferDepth.getShort(byteIndex)
            //            short confidencePixelValue=bufferConfidence.getShort(byteIndex);
            val confidencePixelValue = confidenceBuffer[
                    deepY * confidencePlane.rowStride +
                            deepX * confidencePlane.pixelStride
            ]
            val confidenceNormalised =
                (confidencePixelValue.toInt() and HEX_AND).toDouble() / DIVIDER
            depthImage.close()

            results.add(depthSample.toFloat())
            results.add(confidenceNormalised.toFloat())
            results
            //            short depthSample = buffer.getShort(byteIndex);
            //
            //            // Only the lowest 13 bits are used to represent depth in millimeters.
            //            return (depthSample & 0x1FFF);
        } catch (e: NotYetAvailableException) {
            results.add(-1.0f)
            results.add(0.0f)
            results
            // This normally means that depth data is not available yet.
        }
    }

    companion object {
        //        private val TAG = BackgroundRenderer::class.java.simpleName
        private const val ERROR_VALUE = 0f
        private const val HEX_AND = 0xff
        private const val DIVIDER = 255f

        // components_per_vertex * number_of_vertices * float_size
        private const val COORDS_BUFFER_SIZE = 2 * 4 * 4
        private val NDC_QUAD_COORDS_BUFFER = ByteBuffer.allocateDirect(COORDS_BUFFER_SIZE).order(
            ByteOrder.nativeOrder()
        ).asFloatBuffer()
        private val VIRTUAL_SCENE_TEX_COORDS_BUFFER =
            ByteBuffer.allocateDirect(COORDS_BUFFER_SIZE).order(
                ByteOrder.nativeOrder()
            ).asFloatBuffer()

        init {
            NDC_QUAD_COORDS_BUFFER.put(
                floatArrayOf( /*0:*/
                    -1f, -1f, /*1:*/+1f, -1f, /*2:*/-1f, +1f, /*3:*/+1f, +1f
                )
            )
            VIRTUAL_SCENE_TEX_COORDS_BUFFER.put(
                floatArrayOf( /*0:*/
                    0f, 0f, /*1:*/1f, 0f, /*2:*/0f, 1f, /*3:*/1f, 1f
                )
            )
        }
    }
}
